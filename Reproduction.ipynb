{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from skmultilearn.ensemble import RakelD\n",
    "from bow import BOW"
   ]
  },
  {
   "source": [
    "# Data\n",
    "\n",
    "source: https://github.com/NLeSC/spudisc-emotion-classification\n",
    "\n",
    "Preprocessed versions of the data, split for training and testing a classifier, can be found in the files train.txt and test.txt. These contain one sentence per line with labels at the end of each line. A single space separates the labels from the text. Multiple labels are separated by underscores. Where a sentence received no label, the string None appears. (No label means no emotions assigned by the annotator; all sentences have been annotated.)\n",
    "\n",
    "ToDo: \n",
    "- cite and description of data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILES = Path('./data/')\n",
    "TEST_DATA = DATA_FILES / 'test.txt'\n",
    "TRAIN_DATA = DATA_FILES / 'train.txt'\n",
    "\n",
    "train_sentences, train_labels = BOW().load_data_raw(TRAIN_DATA)\n",
    "test_sentences, test_labels = BOW().load_data_raw(TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(503,)\n(503,)\n(126,)\n(126,)\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_sentences.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "source": [
    "# Features\n",
    "\n",
    "Both algorithms use standard bag-of-words features with stop word removal and optional tf–idf weighting.\n",
    "\n",
    "## BOW and Tf-idf\n",
    "\n",
    "described at: https://medium.com/betacom/bow-tf-idf-in-python-for-unsupervised-learning-task-88f3b63ccd6d\n",
    "\n",
    "scikit BOW: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
    "\n",
    "scit tfid: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\n",
    "\n",
    "### Stop word removal\n",
    "\n",
    "described at: https://aisb.org.uk/wp-content/uploads/2019/12/Final-vol-02.pdf#page=59\n",
    "\n",
    "ToDo\n",
    "- describe how the features are created\n",
    "\n",
    "Difficultys:\n",
    "- what does optional tf-idf mean? (for RAKEL tf-idf weighting with logaritimc tf)\n",
    "- same features for OvR and RAKEL ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BOW(\n",
    "    stop_words=True,\n",
    "    tfidf=True,\n",
    "    log=True,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = bow.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(503, 2168)\n(503, 8)\n(126, 2168)\n(126, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Algorithms\n",
    "\n",
    "## One-vs-Rest\n",
    "\n",
    "Reduction to Binary classifyers\n",
    "\n",
    "implementation of liblinear in sklearn is used which can be found in: https://scikit-learn.org/stable/modules/generated/\n",
    "sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "\n",
    "wrapper: https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier\n",
    "\n",
    "description at: https://scikit-learn.org/stable/modules/svm.html (chapter 1.4.1.1.)\n",
    "\n",
    "ToDo\n",
    "- oversampling\n",
    "- todo find best parameters per emotion and seperatly optimise models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr = OneVsRestClassifier(\n",
    "    LinearSVC(penalty='l1', dual=False, max_iter=10e3))"
   ]
  },
  {
   "source": [
    "## RAKEL\n",
    "\n",
    "### Method\n",
    "\n",
    "Implementation\n",
    "- use skmultilearns RAKELd \n",
    "    - http://scikit.ml/api/skmultilearn.ensemble.rakeld.html\n",
    "    - there the same paper is referenced as in our paper\n",
    "    - what does the d stand for ? \n",
    "- uses linear SVMs as its base learnes\n",
    "    - https://sklearn.org/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "    - fixed regularization parameter C=1\n",
    "- no tuning applyed in paper due to time constraints\n",
    "- use tf-idf weighting with logaritimic tf\n",
    "- automatic oversampling\n",
    "    - account for few occurances of some labels and label subsets\n",
    "- use k=3 for label subsets -> undoing randomization as only 35 size-k subsets of label sets occure\n",
    "\n",
    "\n",
    "ToDo:\n",
    "- automatic oversampling\n",
    "\n",
    "Difficultys:\n",
    "- what implementation used for RAKEL ?\n",
    "- what implementation used for base learner (linear SVM)\n",
    "- not clear if L1 or L2 penalty (use L2 as it is default) ?\n",
    "- how is the oversampling applied ?\n",
    "    - what implementation / algorithm used ?\n",
    "- what to set for base_classifier_require_dense ? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rakel = RakelD(\n",
    "    base_classifier=LinearSVC(C=1),\n",
    "    base_classifier_require_dense=[True, True],\n",
    "    labelset_size=3\n",
    ")"
   ]
  },
  {
   "source": [
    "# Benchmark\n",
    "\n",
    "## Method:\n",
    "\n",
    "Empirically evaluate the algorithms on the dataset\n",
    "- report scores per class and averaged over all classes\n",
    "    - F1 scores\n",
    "    - accuracy\n",
    "        - overall accuracy: one minus the Hamming loss\n",
    "- all scores averaged over ten runs of each training algorithm to account for randomization\n",
    "\n",
    "## Info:\n",
    "- RAKEL results of all runs achive the exact same scores despite randomization\n",
    "- RAKEL does not learn to predict \"Anger\" \n",
    "- OvR archives bad score for four runs and zero for remanaing runs for label \"Anger\"\n",
    "\n",
    "ToDo\n",
    "- set seed\n",
    "- randomize\n",
    "- parameters of OvR is not set in the beginning of experiments but determined during run\n",
    "    - does this happen once or for each run ?\n",
    "- time is also measured but not reported\n",
    "    - we should capture this\n",
    "\n",
    "Difficulties\n",
    "- no seed \n",
    "- how to randomize the runs?\n",
    "    - same seed for both algorithms?\n",
    "    - how to vary seed ?\n",
    "- what is used to calculate the overall scores ?\n",
    "- are OvR hyperparameters optimized once or for every run ? \n",
    "- stated that RAKEL faster but no times reported"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(bow, ovr, rakel, num_runs=10):\n",
    "    results_all_runs = []\n",
    "\n",
    "    X_train, X_test, y_train, y_test = bow.create()\n",
    "    labels = bow.get_labels()\n",
    "\n",
    "    for n in range(num_runs):\n",
    "        # run OvR model\n",
    "        ovr.fit(X_train, y_train)\n",
    "        ovr_prediction = ovr.predict(X_test)\n",
    "        ovr_f1 = f1_score(y_test, ovr_prediction, average=None)\n",
    "        ovr_accuracy = jaccard_score(y_test, ovr_prediction, average=None)\n",
    "\n",
    "        # run RAKEL model\n",
    "        rakel.fit(X_train, y_train)\n",
    "        rakel_prediction = rakel.predict(X_test)\n",
    "        rakel_f1 = f1_score(y_test, rakel_prediction, average=None)\n",
    "        rakel_accuracy = jaccard_score(y_test, rakel_prediction, average=None)\n",
    "\n",
    "        # capture results in DataFrame\n",
    "        results = pd.DataFrame({\n",
    "            'Emotion': labels,\n",
    "            'run': n,\n",
    "            'ovr_f1': ovr_f1,\n",
    "            'ovr_accuracy': ovr_accuracy,\n",
    "            'rakel_f1': rakel_f1,\n",
    "            'rakel_accuracy': rakel_accuracy,\n",
    "        })\n",
    "        results_all_runs.append(results)\n",
    "\n",
    "    return pd.concat(results_all_runs), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, labels = benchmark(bow, ovr, rakel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(80, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    Emotion  run    ovr_f1  ovr_accuracy  rakel_f1  rakel_accuracy\n",
       "0     Anger    0  0.000000      0.000000  0.000000        0.000000\n",
       "1      Fear    0  0.285714      0.166667  0.166667        0.090909\n",
       "2  Interest    0  0.071429      0.037037  0.000000        0.000000\n",
       "3       Joy    0  0.500000      0.333333  0.200000        0.111111\n",
       "4      Love    0  0.500000      0.333333  0.534653        0.364865"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>run</th>\n      <th>ovr_f1</th>\n      <th>ovr_accuracy</th>\n      <th>rakel_f1</th>\n      <th>rakel_accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Anger</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Fear</td>\n      <td>0</td>\n      <td>0.285714</td>\n      <td>0.166667</td>\n      <td>0.166667</td>\n      <td>0.090909</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Interest</td>\n      <td>0</td>\n      <td>0.071429</td>\n      <td>0.037037</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Joy</td>\n      <td>0</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.200000</td>\n      <td>0.111111</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Love</td>\n      <td>0</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.534653</td>\n      <td>0.364865</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Anger', 'Fear', 'Interest', 'Joy', 'Love', 'None', 'Sadness',\n",
       "       'Surprise'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "source": [
    "# Results\n",
    "\n",
    "## Method:\n",
    "\n",
    "\n",
    "ToDo\n",
    "- compute overall results per emotion for all runs\n",
    "    - overall accuracy ?\n",
    "- compute variances"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calulate_results(scores, labels):\n",
    "    grouped = scores.groupby('Emotion').agg(\n",
    "        ovr_f1_mean=('ovr_f1', 'mean'),\n",
    "        ovr_f1_var=('ovr_f1', 'var'),\n",
    "        ovr_accuracy_mean=('ovr_accuracy', 'mean'),\n",
    "        ovr_accuracy_var=('ovr_accuracy', 'var'),\n",
    "        rakel_f1_mean=('rakel_f1', 'mean'),\n",
    "        rakel_f1_var=('rakel_f1', 'var'),\n",
    "        rakel_accuracy_mean=('rakel_accuracy', 'mean'),\n",
    "        rakel_accuracy_var=('rakel_accuracy', 'var'),\n",
    "    )\n",
    "    return grouped.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          ovr_f1_mean    ovr_f1_var  ovr_accuracy_mean  ovr_accuracy_var  \\\n",
       "Emotion                                                                    \n",
       "Anger        0.000000  0.000000e+00           0.000000      0.000000e+00   \n",
       "Fear         0.285714  0.000000e+00           0.166667      0.000000e+00   \n",
       "Interest     0.071182  6.066636e-07           0.036905      1.749671e-07   \n",
       "Joy          0.523077  2.366864e-03           0.355556      2.194787e-03   \n",
       "Love         0.500000  0.000000e+00           0.333333      0.000000e+00   \n",
       "None         0.357143  0.000000e+00           0.217391      0.000000e+00   \n",
       "Sadness      0.000000  0.000000e+00           0.000000      0.000000e+00   \n",
       "Surprise     0.285714  0.000000e+00           0.166667      0.000000e+00   \n",
       "\n",
       "          rakel_f1_mean  rakel_f1_var  rakel_accuracy_mean  rakel_accuracy_var  \n",
       "Emotion                                                                         \n",
       "Anger          0.000000      0.000000             0.000000            0.000000  \n",
       "Fear           0.100000      0.007407             0.054545            0.002204  \n",
       "Interest       0.033391      0.001864             0.017424            0.000508  \n",
       "Joy            0.314639      0.008368             0.189829            0.004141  \n",
       "Love           0.559356      0.001384             0.389109            0.001309  \n",
       "None           0.504291      0.001414             0.337914            0.001114  \n",
       "Sadness        0.000000      0.000000             0.000000            0.000000  \n",
       "Surprise       0.142592      0.007865             0.079042            0.002807  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ovr_f1_mean</th>\n      <th>ovr_f1_var</th>\n      <th>ovr_accuracy_mean</th>\n      <th>ovr_accuracy_var</th>\n      <th>rakel_f1_mean</th>\n      <th>rakel_f1_var</th>\n      <th>rakel_accuracy_mean</th>\n      <th>rakel_accuracy_var</th>\n    </tr>\n    <tr>\n      <th>Emotion</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Anger</th>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Fear</th>\n      <td>0.285714</td>\n      <td>0.000000e+00</td>\n      <td>0.166667</td>\n      <td>0.000000e+00</td>\n      <td>0.100000</td>\n      <td>0.007407</td>\n      <td>0.054545</td>\n      <td>0.002204</td>\n    </tr>\n    <tr>\n      <th>Interest</th>\n      <td>0.071182</td>\n      <td>6.066636e-07</td>\n      <td>0.036905</td>\n      <td>1.749671e-07</td>\n      <td>0.033391</td>\n      <td>0.001864</td>\n      <td>0.017424</td>\n      <td>0.000508</td>\n    </tr>\n    <tr>\n      <th>Joy</th>\n      <td>0.523077</td>\n      <td>2.366864e-03</td>\n      <td>0.355556</td>\n      <td>2.194787e-03</td>\n      <td>0.314639</td>\n      <td>0.008368</td>\n      <td>0.189829</td>\n      <td>0.004141</td>\n    </tr>\n    <tr>\n      <th>Love</th>\n      <td>0.500000</td>\n      <td>0.000000e+00</td>\n      <td>0.333333</td>\n      <td>0.000000e+00</td>\n      <td>0.559356</td>\n      <td>0.001384</td>\n      <td>0.389109</td>\n      <td>0.001309</td>\n    </tr>\n    <tr>\n      <th>None</th>\n      <td>0.357143</td>\n      <td>0.000000e+00</td>\n      <td>0.217391</td>\n      <td>0.000000e+00</td>\n      <td>0.504291</td>\n      <td>0.001414</td>\n      <td>0.337914</td>\n      <td>0.001114</td>\n    </tr>\n    <tr>\n      <th>Sadness</th>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Surprise</th>\n      <td>0.285714</td>\n      <td>0.000000e+00</td>\n      <td>0.166667</td>\n      <td>0.000000e+00</td>\n      <td>0.142592</td>\n      <td>0.007865</td>\n      <td>0.079042</td>\n      <td>0.002807</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "calulate_results(results, labels)"
   ]
  },
  {
   "source": [
    "## Test significance between Algorithms\n",
    "\n",
    "### Method:\n",
    "\n",
    "Welch's one-sided t-test\n",
    "- implementation: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
    "- parameters:\n",
    "    - equal_var=False\n",
    "        - If False, perform Welch’s t-test, which does not assume equal population variance.\n",
    "    - alternative='greater'\n",
    "        - ‘greater’: one-sided\n",
    "\n",
    "\n",
    "Todo\n",
    "- compute Welch’s one-sided t-test\n",
    "    - test significance for alpha = (0.05, 0.001)\n",
    "- optional: compute other test statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_significance(scores, labels):\n",
    "    results = []\n",
    "    for label in labels:\n",
    "        result = {}\n",
    "        \n",
    "        # calculate p-values for F1\n",
    "        ovr_f1 = scores[scores['Emotion'] == label]['ovr_f1'].values\n",
    "        rakel_f1 = scores[scores['Emotion'] == label]['rakel_f1'].values\n",
    "        welch_f1 = ttest_ind(ovr_f1, rakel_f1, equal_var=False, alternative='greater')\n",
    "        result['f1'] = welch_f1.pvalue\n",
    "\n",
    "        # calculate p-calues for Accuracy\n",
    "        ovr_accuracy = scores[scores['Emotion'] == label]['ovr_accuracy'].values\n",
    "        rakel_accuracy = scores[scores['Emotion'] == label]['rakel_accuracy'].values\n",
    "        welch_accuracy = ttest_ind(ovr_accuracy, rakel_accuracy, equal_var=False, alternative='greater')\n",
    "        result['accuracy'] = welch_accuracy.pvalue\n",
    "\n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results, index=labels).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             f1  accuracy\n",
       "Anger       NaN       NaN\n",
       "Fear      0.000     0.000\n",
       "Interest  0.011     0.012\n",
       "Joy       0.000     0.000\n",
       "Love      1.000     1.000\n",
       "None      1.000     1.000\n",
       "Sadness     NaN       NaN\n",
       "Surprise  0.000     0.000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Anger</th>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Fear</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>Interest</th>\n      <td>0.011</td>\n      <td>0.012</td>\n    </tr>\n    <tr>\n      <th>Joy</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>Love</th>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>None</th>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>Sadness</th>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Surprise</th>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "calculate_significance(results, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDO something runs wrong here the p-values for accuracy and f1 are exactly the same and anger and sadness have NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}